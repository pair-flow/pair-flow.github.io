\section{Introduction}
\label{sec:intro}
\vspace{-0.75\baselineskip}

Discrete Flow Models (DFMs)~\citep{Campbell:2024MultiFlow,Gat:2024DFM} have recently emerged as a promising class of generative models, extending the idea of Flow Models (FMs)~\cite{} for continuous data to the discrete domain. By adapting flow-based principles to categorical structures, DFMs provide a principled and efficient way to capture complex discrete distributions through iterative sampling. They have shown success across a variety of applications, particularly in scientific domains such as molecule generation~\citep{ramakrishnan2014quantum,irwin2012zinc}, where DFMs offer a natural framework for modeling chemical structures and generating novel candidates.

Analogous to FMs in the continuous domain, a key challenge of DFMs is the long computation time for generation due to their iterative sampling nature. Recent work~\citep{Deschenaux:2025sdtt,Hayakawa:2025di4c,Sahoo:2025Duo,Yoo:2025ReDi} have sought to accelerate the generative process through distillation-based finetuning, which builds on ideas originally developed for continuous flow matching. Notably, ReFlow~\citep{Liu:2023RF} is a well-known technique for FMs that pairs samples from the source (prior) distribution and the target distribution by running the generative process of a pretrained model and using the resulting pairs for finetuning. Recently, this idea has also been extended to DFMs~\citep{Yoo:2025ReDi} to reduce conditional total correlation through finetuning, thereby enabling few-step generation.

Despite these promising results in acceleration, distillation-based methods incur substantial finetuning overhead, amounting to about 10–20\% of the time required to train the base model from scratch. In other words, the gain in generation speed comes at the expense of considerable additional training cost. To our knowledge, no prior work has addressed this training-time cost when pursuing inference-time acceleration. This raises a natural question: can we achieve speedups comparable to distillation-based approaches while requiring only a lightweight preprocessing phase that requires orders of magnitude less compute, on the order of tens of GPU minutes?

We propose~\ourname{}, a training framework for DFMs that enables few-step sampling by constructing paired source–target samples using closed-form velocities.~\textbf{While inspired by ReDi-style coupling-driven training, our approach eliminates the need for a pretrained teacher by using closed-form formulations and achieves acceleration without finetuning.} The algorithm for computing source–target pairs is fully parallelizable and requires at most 1.7\% of compute needed for full model training. Despite relying only on a lightweight preprocessing step,~\ourname{} attains performance comparable to, or even superior to state-of-the-art distillation-based techniques, which can require up to 143 times more computation.
% See Tab. 1, on QM9, our method is 138 times faster than DCD.
Furthermore, models trained with our technique provide stronger bases for subsequent distillation, delivering additional performance gains while incurring only minimal preprocessing cost.

At the core of our framework is the simulation of probability paths connecting source (prior) and target (data) distributions in discrete spaces, made possible by closed-form expressions of velocities. While closed-form forward velocities have been studied for flow models in continuous domains~\citep{Karras:2022EDM,Bertrand:2025Closed}, they have, to the best of our knowledge, neither been explored for DFMs nor applied to identifying suitable source–target pairs in the context of distillation-based acceleration, as in ReDi~\citep{Yoo:2025ReDi}. In this work, we investigate this idea for the first time.
%
For DFMs, with a particular focus on uniform-state models~\citep{Sahoo:2025Duo,Schiff:2025UDLM} equipped with a self-correcting mechanism, we show that the closed-form forward velocity is determined by the Hamming distance, which measures the number of differing tokens between two sequences. Using this velocity, samples from the source (latent) distribution can be mapped to given target samples. However, because multiple source samples may map to the same target, covering all targets through coupling would require an impractically large number of source samples. To overcome this, we derive the corresponding backward velocity in closed form and leverage it to simulate backward probability paths that efficiently map data points to source points, making pair discovery computationally efficient.

In our experiments, we show that the proposed framework enables few-step sampling across diverse discrete domains, including molecular data~\citep{ramakrishnan2014quantum,irwin2012zinc} and 2D images, exemplified by MNIST-Binary~\citep{lecun2002gradient} and CIFAR-10~\citep{krizhevsky2009learning}. On the QM9~\citep{ramakrishnan2014quantum} and ZINC-250k~\citep{irwin2012zinc} datasets,~\textbf{~\ourname{} not only improves the base model but also performs comparably to, or even better than, distilled models that require up to $143\times$ more compute during finetuning, compared to our lightweight preprocessing algorithm.} Similar improvements are observed on MNIST-Binary, where models paired with~\ourname{} achieve performance comparable to those using DCD~\citep{Sahoo:2025Duo} and ReDi~\citep{Yoo:2025ReDi}, while being up to $35\times$ faster. Furthermore, after subsequent distillation, base models trained with pairs generated by our method consistently achieve higher performance, underscoring the importance of well-constructed source–target pairings.
