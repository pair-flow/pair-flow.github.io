% \vspace{-0.5\baselineskip}

\section{Preliminaries}
\label{sec:preliminary}
\vspace{-0.75\baselineskip}

In this section, we provide a brief overview of flow matching for generative modeling of discrete data (\Secref{subsec:flow_matching}), followed by a rectification technique~\citep{Yoo:2025ReDi} that enables faster generation (\Secref{subsec:rectified_flow}) by reducing total correlation errors.

% ----------------------------------------------------
\subsection{Discrete Flow matching}
\label{subsec:flow_matching}
\vspace{-0.5\baselineskip}

The goal of Discrete Flow Matching (DFM)~\citep{Campbell:2024MultiFlow,Gat:2024DFM} is to learn a probability path $p_t(\cdot)$ that connects a known, easy-to-sample source distribution $p(x)$ to an unknown target distribution $q(x)$, both defined over a discrete state space. Once $p_t(\cdot)$ is known, samples from $q$ can be generated by drawing $x_0 \sim p$ and transporting it along the path. 

Specifically, consider a sequence $x = (x^1, x^2, \ldots, x^N)$ of $N$ tokens, where each token takes values in a vocabulary $\mathcal{V} = \{1, 2, \dots, K\}$ of size $K$. A sequence $x$ then resides in the product space $\mathcal{V}^N$. We denote by $\Delta^K = \{ p \in \mathbb{R}^K \vert \sum_i p_i=1, p_i \geq 0\}$ the probability simplex of dimension $K-1$, on which distributions over $\mathcal{V}$ are defined.

Given a target probability path $p_t(x): \mathcal{V}^N \times [0,1] \to [0,1] $ with an associated velocity field $v_t(x): \mathcal{V}^N \times [0,1] \to \mathbb{R}^{N\times K}$, we introduce a network $p_{1 \vert t}^{\theta}(x): \mathcal{V}^N \times [0,1] \to (\Delta^K)^N$ to approximate $v_t(x)$.
%%%%
Its parameters $\theta$ are optimized via the DFM objective~\citep{Gat:2024DFM}:
\begin{align}
    \mathcal{L}_{\text{DFM}}(\theta) = - \sum_{i \in \{1,\dots,N\}} \mathbb{E}_{t \sim \mathcal{U}[0,1], x_0 \sim p, x_1 \sim q, z \sim p_t(x|x_0, x_1)} \log p_{1 \vert t}^\theta (x_1^i \vert z),
    \label{eqn:dfm_loss}
\end{align}
where $p_{1 \vert t}^\theta(x_1^i \vert z)$ denotes the learned probability denoiser, which predicts the categorical distribution of the clean token $x_1^i$ given an intermediate sequence $z$.
%
Here, the conditional probability path $p_t(z \vert x_0, x_1)$ generates samples $z$ by interpolating between a data point $x_1~\sim q$ and a source sample $x_0~\sim p$. Assuming independence across tokens in sequence $x$, the conditional density factorizes as
\begin{align}
    p_t(z \vert x_0, x_1) = \prod_{i=1}^N p_t(z^i \vert x_0, x_1).
    \label{eqn:p_t_factorize}
\end{align}
%
As token-wise conditional paths $p_t(z^i \vert x_0, x_1)$,~\citet{Gat:2024DFM} employ the mixture path of form:
\begin{align}
    p_t(z^i|x_0, x_1) = (1-\kappa_t)\delta_{x_0}(z^i) + \kappa_t\delta_{x_1}(z^i),
\end{align}
where the~\emph{scheduler} $\kappa_t = \kappa(t)$ is a monotonically increasing function over $t \in [0,1]$ satisfying $\kappa_0 = 0$ and $\kappa_1 = 1$. For notational convenience, given $x, y \in \mathcal{V}^N$, we define the Dirac delta $\delta_y(x)$ as
\begin{align}
    \delta_y (x) = \prod_{i=1}^N \delta_{y^i} (x^i), \,\, \text{where } \delta_{y^i} (x^i) = \begin{cases} 1 & x^i=y^i \\ 0 & x^i \neq y^i \end{cases}.
\end{align}
We also use the shorthand $\delta_y(x^i) = \delta_{y^i}(x^i)$ .
%
After optimizing $\theta$, the learned model parameterizes an approximation of the marginal velocity field:
%%%%
\begin{align}
    v_t^{\theta}(x^i, z) = \frac{\dot\kappa_t}{1 - \kappa_t} \left[p^{\theta}_{1|t}(x^i|z) - \delta_{z}(x^i) \right],
\end{align}
where $\dot{\kappa}_t=\frac{\partial \kappa_t}{\partial t}$.
%%%%
This learned velocity field $v_t^{\theta}(x^i, z)$ then transports samples over the interval $[0, 1]$ to simulate trajectories along $p_t(\cdot)$ and thereby generate samples. Each update step is defined as:
\begin{align}
    x_{t+h}^i \sim \text{Cat} \left(x_{t+h}^i; \delta_{x_t^i}(\cdot) + h \cdot v^\theta_t(x_{t+h}^i, x_t) \right),
    \label{eqn:sampling_step}
\end{align}
where $h > 0$ is the step size.

\subsection{Straightening Probability Paths for Accelerated Sampling}
\label{subsec:rectified_flow}
\vspace{-0.5\baselineskip}

The concept of straight probability paths was originally introduced in the continuous domain to enable accelerated sampling. Prior work~\citep{Liu:2023RF} identified curved probability paths as a key challenge in few-step sampling: when velocity fields are evaluated only at coarse time steps, numerical integration deviates from the true trajectories. \citet{Liu:2023RF} addressed this issue through ``rectification,'' in which a student flow model is trained on source--target pairs generated by a teacher model, effectively yielding significantly straighter probability paths.

In the discrete setting, this challenge of \textit{path curvature} translates to capturing the \textit{statistical correlations} between tokens. Since DFMs approximate exponentially large joint transitions through fully factorized per-token updates, a mismatch inevitably arises between the true joint transition and its product-form approximation. This discrepancy becomes especially detrimental during few-step generation, where highly correlated tokens must be updated simultaneously. To address this, prior works have primarily relied on distillation-based approaches~\citep{Hayakawa:2025di4c,Sahoo:2025Duo,Deschenaux:2025sdtt}, aiming to better capture these correlations by explicitly transferring multi-step dependencies from a stronger teacher model.

\citet{Yoo:2025ReDi} formalized this factorization mismatch via conditional Total Correlation (TC), defined as:
\begin{align}
    \text{TC}_{\pi}(x_s|x_t) = \mathbb{E}_{x_t} \left[ D_{\mathrm{KL}} \left( p_{s|t}(x_s|x_t) \Vert \prod_{i=1}^{N} p_{s|t}(x_s^i|x_t) \right) \right],
    \label{eqn:redi_tc}
\end{align}
which serves as a metric for the factorization error. Crucially, \citet{Yoo:2025ReDi} interpret this factorization error as the discrete analog of path curvature: minimizing TC is equivalent to ``straightening'' the trajectory by decoupling token transitions. Analogous to ReFlow~\citep{Liu:2023RF}, which rectifies continuous paths, they demonstrate that reducing~\Eqref{eqn:redi_tc} requires iteratively refining the source--target coupling $\pi(x_0, x_1)$. To achieve this, they employ an iterative distillation process, alternating between generating improved pairs using the current model and optimizing $\mathcal{L}_{\text{DFM}}$. This procedure effectively finds a ``statistically straight'' coupling that enables efficient few-step generation.
