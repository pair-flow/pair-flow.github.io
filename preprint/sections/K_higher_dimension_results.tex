
\section{Application for more Complex Systems}
\label{sec:higher_dimension_results}

In this section, we evaluate our method on a higher-dimensional dataset. Specifically, we use the FFHQ~\citep{karras2019styleffhq} dataset, downsampled to $64\times64$. Following the same protocol as in our main experiments, we generate 5{,}000 samples across varying timesteps and report the FID computed against the training set. The results of this experiment are provided in~\Tabref{tab:ffhq_fid}. All training hyperparameters are kept identical to those used in the CIFAR-10 experiments described in Section~\ref{sec:results}.

\input{tables/103_ffhq64_results}

We adopt the LM1B~\citep{chelba2013lm1b} dataset to evaluate our method under a substantially larger vocabulary size and training corpus. The text corpus is segmented into sequences of varying lengths ($N = 16, 32, 64, 128$), while keeping the total number of training samples fixed ($|X_1| \approx 3.5\text{M}$). To assess generation quality, we compute generative perplexity using GPT-2 Large and entropy on 1,024 generated samples for each NFE setting. The results are summarized in~\Tabref{tab:lm1b_gen_ppl} and~\Tabref{tab:lm1b_entropy}. For training, we follow the network hyperparameter configuration of~\citep{Schiff:2025UDLM}, modifying only the number of training iterations for each sequence dimensionality.

\input{tables/104_lm1b_results}