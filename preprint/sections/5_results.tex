\vspace{-0.5\baselineskip}
\section{Experimental Results}
\label{sec:results}
\vspace{-0.5\baselineskip}
%%%%
We validate the effectiveness of the proposed method and the source–target pairs it discovers across several discrete generative modeling benchmarks involving molecular data and images. We first summarize the experimental setup in~\Secref{subsec:experiment_setting}. In~\Secref{subsec:result_molecular} and~\Secref{subsec:result_image}, we compare our method against baselines in molecular and image generation, respectively. In~\Secref{subsec:result_distilation}, we further demonstrate that models trained with pairs discovered by our method not only achieve improved performance directly, but also benefit subsequent distillation phases, as the resulting base model provides a stronger initialization for existing distillation techniques.


% % ----------------------------------------------------
\vspace{-0.5\baselineskip}
\subsection{Experiment Setup}
\label{subsec:experiment_setting}
%%%%
\input{tables/21_computation_cost}
%%%%
\vspace{-0.5\baselineskip}
\paragraph{Baselines.} 
Across multiple benchmarks, we compare our approach against state-of-the-art discrete flow models, including MDLM~\citep{Sahoo:2024MDLM} and UDLM~\citep{Schiff:2025UDLM}. Since our method is based on a uniform source distribution, we adopt UDLM~\citep{Schiff:2025UDLM}, the leading uniform-state model, as our base  and denote UDLM trained with pairs generated by~\Algref{alg:coupling_backward} as~\ourname{} throughout the remainder of this section. In addition, we compare against these models augmented with distillation-based techniques that require additional finetuning, Discrete Consistency Distillation (DCD)~\citep{Sahoo:2025Duo} and ReDi~\citep{Yoo:2025ReDi}, denoted throughout this section by the suffixes ``+ DCD'' and ``+ ReDi''. 
%
The detailed training setups of these models, such as hyperparameters, are provided in App.~\ref{sec:appendix_results_detail}.
%
Additionally, we report the performance of the same base model trained on pairs formed by each data point and a source sample randomly drawn from the uniform distribution with our detailed experimental results in App.~\ref{sec:appendix_results_full}.

\vspace{-0.5\baselineskip}
\paragraph{Benchmarks.}
We evaluate our method across a diverse set of discrete generative modeling benchmarks, covering both molecule and image generation tasks. For molecule generation, we experiment with the QM9~\citep{ramakrishnan2014quantum} and ZINC-250k~\citep{irwin2012zinc} datasets. For image generation, we use the MNIST dataset~\citep{lecun2002gradient} with binarized pixel values (denoted MNIST-Binary) and the CIFAR-10 dataset~\citep{krizhevsky2009learning}, where pixel intensities are scaled to 8-bit integers, and horizontal flip augmentation is applied. Dataset statistics, including sample size, vocabulary size, and overall dataset size, are summarized in~\Tabref{tab:computation_cost_table}.
\vspace{-0.5\baselineskip}
\paragraph{Evaluation Setup.}
For molecular generation, we follow~\citet{Schiff:2025UDLM} and evaluate the validity, uniqueness, and novelty of generated molecules. Specifically, we sample 1,024 SMILES strings~\citep{weininger1988smiles}, convert them into molecular graphs, and compute these metrics. All results are averaged over 10 trials, with further details provided in App.~\ref{sec:appendix_results_full}. For image generation, we report Fréchet Inception Distance (FID)~\citep{Heusel:2017FID} and Inception Score (IS)~\citep{Salimans:2016IS}. FID is computed with 1,000 images for MNIST-Binary, and both FID and IS are computed with 5,000 generated images for CIFAR-10. The training dataset is used as the reference for FID computation.
%
Across all experiments, we vary the number of sampling steps to evaluate performance in both low- and high-NFE settings. In particular, we generate samples using $1-64$ steps for molecular benchmarks (QM9 and ZINC-250k) and MNIST-Binary benchmark, and $8-1024$ steps for CIFAR-10~\citep{krizhevsky2009learning}, as models yielded excessively high FIDs under extremely low-step settings.

% % ----------------------------------------------------

%\vspace{-0.5\baselineskip}
\subsection{Molecule Generation}
\label{subsec:result_molecular}
\vspace{-0.5\baselineskip}
%%%%
We begin by benchmarking unconditional molecule generation, where models are tasked with generating SMILES strings~\citep{weininger1988smiles} that represent molecules. As illustrated in~\Figref{fig:qm9_discrete_plots} and~\Figref{fig:zinc250k_discrete_plots}, which summarize validity (left), uniqueness (middle), and novelty (right),~\ourname{} consistently improves upon its base model UDLM~\citep{Schiff:2025UDLM}, yielding substantial gains in few-step settings.
%
It facilitates 1-step generation on QM9~\citep{ramakrishnan2014quantum}, a challenging setting that requires capturing all token-wise dependencies simultaneously. In this case, validity increases from $17.5$ to $223.4$, corresponding to a $12.8 \times$ improvement.
%
Similar trends are observed in the 2-step and 4-step settings, with validity improving by $231\%$ and $47.6\%$, respectively. As shown in~\Figref{fig:qm9_discrete_plots} (left), this improvement is particularly significant: the 2-step and 4-step validities of~\ourname{} are comparable to the 4-step and 8-step validities achieved by UDLM~\citep{Schiff:2025UDLM}.
Comparable improvements are also seen on the ZINC-250k~\citep{irwin2012zinc} dataset.
%%%%

%%%%
Remarkably,~\textbf{\ourname{} introduces minimal overhead—less than $2\%$ of the training cost as shown in~\Tabref{tab:computation_cost_table}—and requires no pretrained models, yet achieves performance comparable to, and in some cases surpassing, models distilled from the same base using DCD~\citep{Sahoo:2025Duo} and ReDi~\citep{Yoo:2025ReDi}}, both of which rely on pretrained models and finetuning. On both QM9~\citep{ramakrishnan2014quantum} and ZINC-250k~\citep{irwin2012zinc},~\ourname{} consistently outperforms $\text{UDLM + ReDi}$ across all few-step settings, achieving substantially higher 2-step validities on QM9 ($232.4$ vs.\ $416.0$) and ZINC-250k ($75.9$ vs.\ $146.3$). At the same time,~\ourname{} matches the performance of $\text{UDLM + DCD}$, with comparable 2-step validities on QM9 ($416.0$ vs.\ $530.8$). This is particularly notable given that the additional preprocessing cost of~\ourname{} amounts to only $0.69\%$ on QM9~\citep{ramakrishnan2014quantum} and $6.16\%$ on ZINC-250k~\citep{irwin2012zinc}, relative to the full cost of DCD~\citep{Sahoo:2025Duo}. Detailed numerical results with standard deviations are reported in App.~\ref{sec:appendix_results_full}.  

% % ----------------------------------------------------
\input{figures/latex_src/qm9_quanti}
\input{figures/latex_src/zinc250_quanti}
% % ----------------------------------------------------

\vspace{-0.25\baselineskip}
\subsection{Image Generation}
\label{subsec:result_image}
\vspace{-0.5\baselineskip}
%%%%
\input{figures/latex_src/image_quanti}
\input{figures/qualitative2/qualitative}

We further extend our experiments to image domains where each pixel has discretized intensities. As in~\Secref{subsec:result_molecular}, we evaluate model performance across multiple sampling steps and summarize the results in~\Figref{fig:image_discrete_plots}. Qualitative samples for MNIST-Binary and CIFAR-10 are shown in~\Figref{fig:image_qualitative}. Both qualitative and quantitative results show that~\ourname{} improves the performance of UDLM~\citep{Schiff:2025UDLM} and, in few-step settings, achieves performance comparable to DCD~\citep{Sahoo:2025Duo}.

On MNIST-Binary (\Figref{fig:image_discrete_plots}, left),~\ourname{} achieves an FID of $40.59$ in the 1-step setting, equivalently a $68.9\%$ improvement over UDLM~\citep{Schiff:2025UDLM}. Consistent gains are observed across other few-step settings as well: at 2 steps, FID is reduced by $63.3\%$ ($15.61$ vs.\ $42.54$), and at 4 steps by $24.4\%$ ($8.51$ vs.\ $11.25$). On CIFAR-10~\citep{krizhevsky2009learning}, where FID~\citep{Heusel:2017FID} and IS~\citep{Salimans:2016IS} are reported in~\Figref{fig:image_discrete_plots} (middle and right),~\ourname{} likewise outperforms the base UDLM, validating the effectiveness of the discovered source–target pairs.

As in molecular generation (\Secref{subsec:result_molecular}),~\textbf{\ourname{} performs comparable to distillation-based acceleration methods.} On MNIST-Binary, it achieves a lower FID than $\text{UDLM+DCD}$ in the 1-step setting ($40.59$ vs.\ $53.84$) and comparable performance at 2 steps ($15.61$ vs.\ $16.09$). Likewise,~\ourname{} performs competitively with $\text{UDLM+ReDi}$ at 2 steps ($15.61$ vs.\ $10.36$), while requiring substantially less compute than both. As summarized in~\Tabref{tab:computation_cost_table}, DCD~\citep{Sahoo:2025Duo} requires 40 minutes ($T_{\text{DCD}}$) and ReDi~\citep{Yoo:2025ReDi} takes 49 minutes, whereas the preprocessing phase of~\ourname{} completes in just $1.4$ minutes ($T_{\text{\ourname{}}}$), yielding $28.6\times$ and $35\times$ speedups, respectively.
%
On the CIFAR-10 benchmark, both DCD~\citep{Sahoo:2025Duo} and ReDi~\citep{Yoo:2025ReDi} degrade model performance, as indicated by the higher FID in~\Figref{fig:image_discrete_plots} (middle) and lower IS in~\Figref{fig:image_discrete_plots} (right). The results in~\Tabref{tab:cifar10_results} suggest that, overall, acceleration methods do not work well on CIFAR-10. We hypothesize that this issue arises from the low performance of the teacher model, which negatively affects the student model when applying acceleration methods. Detailed results are reported in App.~\ref{sec:appendix_results_full}.  

% %------------------------------------------------------------------------
\vspace{-0.25\baselineskip}
\subsection{Distilling Models Trained with Aligned Pairs}
\label{subsec:result_distilation}
\vspace{-0.5\baselineskip}
%%%%
\input{figures/latex_src/distillation_quanti}
%%%%
While~\ourname{} alone achieves performance comparable to, or even exceeding, distillation-based techniques~\citep{Yoo:2025ReDi,Sahoo:2025Duo}, as shown in~\Secref{subsec:result_molecular} and~\Secref{subsec:result_image}, we further emphasize that it also serves as a strong initialization for subsequent distillation, yielding even greater performance gains when combined with existing methods. Crucially, this incurs negligible additional cost relative to the overall time required for distillation.

We validate this by distilling~\ourname{}, trained on QM9~\citep{ramakrishnan2014quantum}, ZINC-250k~\citep{irwin2012zinc}, and MNIST-Binary, using DCD~\citep{Sahoo:2025Duo} and ReDi~\citep{Yoo:2025ReDi}, and comparing their performance against distilled models whose teachers were the base UDLM~\citep{Schiff:2025UDLM}.
%
As shown in~\Figref{fig:image_distillation_plots},~\textbf{student models distilled from~\ourname{}, denoted~\ourname{}+DCD and~\ourname{}+ReDi, push the frontier of performance previously achieved by distillation-based techniques.} For example, on the QM9 dataset~\citep{ramakrishnan2014quantum},~\ourname{}+DCD substantially improves validity over UDLM+DCD ($453.8$ vs.\ $323$ for 1 step, $685.8$ vs.\ $530.8$ for 2 steps). A similar trend is observed for~\ourname{}+ReDi on ZINC-250k~\citep{irwin2012zinc}, yielding higher scores in both 1-step ($46.3$ vs.\ $0.7$) and 2-step ($221.5$ vs.\ $75.9$) generation. Importantly, as summarized in~\Tabref{tab:computation_cost_table}, these gains are achieved at only minimal additional preprocessing cost: $3.15\%$ of the average runtime of distillation on MNIST-Binary, $0.77\%$ on QM9, and $6.42\%$ on ZINC-250k.

