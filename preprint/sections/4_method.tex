% \vspace{-0.5\baselineskip}
\section{PairFlow}
\label{sec:background}
\vspace{-0.5\baselineskip}

For DFMs, ReDi~\citep{Yoo:2025ReDi} improves sample quality in few-step generation by rectifying source-target pairs. However, it relies on samples from a pretrained model followed by costly retraining or finetuning. We take this one step further and pose the following question: What if these pairs could be generated directly from the data, without relying on a pretrained model or sampling from it?

To address this question, we propose a principled approach for discovering well-aligned source–target pairs without relying on pretrained models, enabling models trained on such pairs to achieve strong performance with few-step sampling. Our method, termed~\ourname{}, leverages closed-form velocity fields that can be computed directly from the data samples, requiring only prior knowledge of the source distribution. We assume this distribution to be uniform, a choice extensively studied in recent work~\citep{Sahoo:2025Duo,Schiff:2025UDLM}, as models trained under this prior naturally acquire self-correcting properties.

In~\Secref{subsec:closed_form_forward_velocity}, we introduce the closed-form forward velocity for discrete flow matching~\citep{Gat:2024DFM}. In~\Secref{subsec:closed_form_backward_velocity}, we extend this to the closed-form backward velocity and propose an algorithm for discovering well-aligned source–target pairs during the preprocessing phase.

% ----------------------------------------------------

\subsection{Finding Pairs via Closed-Form Forward Velocity Fields}
\label{subsec:closed_form_forward_velocity}
\vspace{-0.5\baselineskip}

As discussed in~\Secref{subsec:flow_matching}, DFMs~\citep{Campbell:2024MultiFlow,Gat:2024DFM} aim to learn a marginal velocity field $v_t(\cdot)$ that induces a probability path $p_t(\cdot)$, transporting the source distribution $p_0(\cdot)$ to the target distribution $q(\cdot) = p_1(\cdot)$, which is unknown in practice. Instead, we only have access to a finite dataset of $M$ samples $\{d_m\}_{m=1}^M$. This empirical distribution $\tilde{q}(x)$ can be represented as a mixture of Dirac deltas centered at the observed samples:
\begin{equation}
    q(x) \approx \tilde{q}(x) = \frac{1}{M} \sum_{m=1}^M \delta_{d_m}(x).
    \label{eqn:empirical_target}
\end{equation}
%%%%
For continuous domains,~\citet{Karras:2022EDM,Bertrand:2025Closed} have shown that the velocity field transporting $p_0$ to $q$ can be derived in closed form when both distributions admit tractable density expressions. To the best of our knowledge, this idea has not been explored in discrete domains; in the following, we derive the closed-form velocity field for discrete domains for the first time.

We base our framework on the assumption of a uniform prior distribution over the discrete state space $\mathcal{V}^N$, defined as $p_0(x) = \mathcal{U}^N$, where $\mathcal{U} = \text{Cat}\left(\cdot;\frac{\mathbf{1}}{K}\right)$ denotes the uniform distribution over the dictionary $\mathcal{V}$.
For the empirical target distribution $\tilde{q}(x)$ introduced in~\Eqref{eqn:empirical_target}, we show in App.~\ref{subsec:appx_proof_forward_velocity_discrete} that the closed-form denoiser $p_{1 \vert t}(x^i \vert z)$ and its associated velocity field $\hat{v}_t(x^i, z)$ are given by:

\begin{align}
    p_{1|t}(x^i|z) = \cfrac{\sum_{m=1}^M \delta_{d_{m}^i}(x^i) \gamma^{- h(d_{m}, z)}}{ \sum_{m=1}^M \gamma^{- h(d_{m}, z)}} \quad \Longrightarrow \quad 
    \hat{v}_t(x^i, z) = \frac{\dot\kappa_t}{1 - \kappa_t} \left[p_{1|t}(x^i|z) - \delta_{z}(x^i) \right]
    \label{eqn:closed_forward_velocity}
\end{align}

where $\gamma = (1 + (K-1)\kappa_t)/(1 - \kappa_t)$, $K$ denotes the vocabulary size, and $h(s, z) = N - \sum_{i=1}^N \delta_{s^i}(z^i)$ is the Hamming distance between sequences $s$ and $z$,~\ie{} the number of positions at which they differ. 
%
The token-wise denoiser $p_{1 \vert t}(x^i \vert z)$ above is a weighted mixture of Dirac deltas, where sequences closer to $z$ under the Hamming distance contribute more.
Intuitively, the forward velocity field $\hat{v}_t(x^i, z)$ pulls each token toward those from dataset sequences most similar to $z$.
%
The most direct way to construct source-target pairs using $\hat{v}_t(x^i, z)$ is to sample $x_0 \sim p_0(x)$ and evolve it along the velocity field until it reaches a dataset point $x_1$. In practice, however, the generated data points fail to fully cover $\tilde{q}(x)$, requiring an impractically large number of source samples to achieve sufficient coverage. Our empirical results, reported in App.~\ref{subsec:coverage_forward_velocity}, support this claim and motivate the exploration of a more efficient alternative, which we present in the following section.
%%%%
% Algorithm 1.
\input{algorithms/closed_form_forward}

% ----------------------------------------------------

\subsection{Finding Pairs via Closed-Form Backward Velocity Fields}
\label{subsec:closed_form_backward_velocity}
\vspace{-0.5\baselineskip}

We address this issue by \emph{backtracing} trajectories along $p_t(\cdot)$, starting from $\tilde{q}(x)$ and progressing toward the source distribution $p_0(\cdot)$. Unlike the forward construction in~\Secref{subsec:closed_form_forward_velocity}, this guarantees that all data points in $\tilde{q}(x)$ are included in the resulting pairs by design.
As illustrated at the top of~\Figref{fig:pairflow_method},~\ourname{} inverts data samples toward the source distribution, assumed to be uniform. 
Unlike the standard corruption process used by UDLM~\citep{Schiff:2025UDLM} shown at the bottom of~\Figref{fig:pairflow_method}, the source samples obtained by~\ourname{} remain closer to the original data in terms of Hamming distance. This helps the model learn to recover data with fewer token transitions during training, effectively approximating the straight probability paths explored in ReFlow~\citep{Liu:2023RF} and ReDi~\citep{Yoo:2025ReDi}.

The remaining challenge is to derive the closed-form backward velocity that governs this process. This can be obtained by following a construction analogous to~\Secref{subsec:closed_form_forward_velocity}. Specifically, we first derive the closed-form noise predictor $p_{0 \vert t}(x^i \vert z)$:
\begin{align}
    p_{0|t}(x^i|z) = \delta_{z}(x^i) - \frac{\kappa_t(K \delta_{x^i}(z^i) - 1)}{1 + (K-1)\kappa_t}  \frac{\sum_{m=1}^M  \delta_{d_m^i}(z^i)\gamma^{-h(d_m, z)}}{\sum_{m=1}^M \gamma^{-h(d_m, z)}},
    \label{eqn:closed_form_noise_predictor}
\end{align}
with a detailed derivation provided in App.~\ref{subsec:appx_proof_backward_velocity_discrete}. Substituting this into the definition of the backward velocity field from~\citet{Gat:2024DFM}
\begin{align}
    \label{eqn:backward_velocity}
    \check{v}_t(x^i, z) = \frac{\dot\kappa_t}{\kappa_t} \left[ \delta_z(x^i) - p_{0 \vert t} (x^i \vert z) \right],
\end{align}
we obtain the desired closed-form expression
\begin{align}
    \check{v}_t(x^i, z) = \frac{\dot{\kappa}_t(K \delta_{x^i}(z^i)-1)}{1 + (K-1)\kappa_t}  \frac{\sum_{m=1}^M \delta_{d_m^i}(z^i)\gamma^{-h(d_m, z)}}{\sum_{m=1}^M \gamma^{-h(d_m, z)}}.
    \label{eqn:closed_backward_velocity}
\end{align}

The second term in~\Eqref{eqn:closed_form_noise_predictor} computes the conditional likelihood of the $i$-th token taking value $x^i \in \mathcal{V}$ given the current sequence $z$, marginalized over all dataset $\{d_m\}_{m=1}^M$. The contribution of each data sample $d_m$ is determined by its proximity to $z$ under the Hamming distance $h(d_m, z)$, assigning higher weight to tokens with greater local consensus. Consequently, updating with $\check{v}_t(x^i, z)$ pushes the sample away from the data distribution and toward the source distribution $p_0(x)$.
%
Using $\check{v}_t(x^i, z)$, we construct pairs $\{(x_{0,m}, x_{1,m})\}_{m=1}^{M}$ by initializing from data points $\{d_{m}\}_{m=1}^M$ (equivalently, $\left\{x_{1,m} \right\}_{m=1}^M$) and iteratively applying the backward update rule in~\Eqref{eqn:sampling_step} for a fixed number of iterations $T$. The overall procedure is summarized in~\Algref{alg:coupling_backward}.